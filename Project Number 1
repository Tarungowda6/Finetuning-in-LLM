1. Setup Development Environment
   
In this example, we use the PyTorch Deep Learning AMI with already set up CUDA drivers and
PyTorch installed. We still have to install the Hugging Face Libraries, including transformers and
datasets. Running the following cell will install all the required packages

# install Hugging Face Libraries
!pip install "peft==0.2.0"
!pip install "transformers==4.27.2" "datasets==2.9.0" "accelerate==0.17.1" "evaluate==0.4.0" "bitsandbytes==0.37.1"
loralib --upgrade –quiet
# install additional dependencies needed for training
!pip install rouge-score tensorboard py7zr

Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colabwheels/public/simple/
Requirement already satisfied: peft==0.2.0 in /usr/local/lib/python3.10/dist-packages
(0.2.0)
Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from
peft==0.2.0) (1.22.4)
Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages
(from peft==0.2.0) (23.1)
Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from
peft==0.2.0) (5.9.5)
Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from
peft==0.2.0) (6.0)
Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages
(from peft==0.2.0) (2.0.1+cu118)
Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages
(from peft==0.2.0) (4.27.2)
Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from
peft==0.2.0) (0.17.1)
Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from
torch>=1.13.0->peft==0.2.0) (3.12.0)
Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/distpackages (from torch>=1.13.0->peft==0.2.0) (4.5.0)
Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from
torch>=1.13.0->peft==0.2.0) (1.11.1)
Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from
torch>=1.13.0->peft==0.2.0) (3.1)
Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from
torch>=1.13.0->peft==0.2.0) (3.1.2)
Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages
(from torch>=1.13.0->peft==0.2.0) (2.0.0)
Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from
triton==2.0.0->torch>=1.13.0->peft==0.2.0) (3.25.2)
Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from
triton==2.0.0->torch>=1.13.0->peft==0.2.0) (16.0.5)
Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in
/usr/local/lib/python3.10/dist-packages (from transformers->peft==0.2.0) (0.15.1)
Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/distpackages (from transformers->peft==0.2.0) (2022.10.31)
Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from
transformers->peft==0.2.0) (2.27.1)
Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in
/usr/local/lib/python3.10/dist-packages (from transformers->peft==0.2.0) (0.13.3)
Requirement already sati Looking in indexes: https://pypi.org/simple, https://uspython.pkg.dev/colab-wheels/public/simple/
Requirement already satisfied: peft==0.2.0 in /usr/local/lib/python3.10/dist-packages
(0.2.0)
Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from
peft==0.2.0) (1.22.4)
Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages
(from peft==0.2.0) (23.1)
Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from
peft==0.2.0) (5.9.5)
Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from
peft==0.2.0) (6.0)
Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages
(from peft==0.2.0) (2.0.1+cu118)
Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages
(from peft==0.2.0) (4.27.2)
Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from
peft==0.2.0) (0.17.1)
Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from
torch>=1.13.0->peft==0.2.0) (3.12.0)
Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/distpackages (from torch>=1.13.0->peft==0.2.0) (4.5.0)
Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from
torch>=1.13.0->peft==0.2.0) (1.11.1)
Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from
torch>=1.13.0->peft==0.2.0) (3.1)
Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from
torch>=1.13.0->peft==0.2.0) (3.1.2)
Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages
(from torch>=1.13.0->peft==0.2.0) (2.0.0)
Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from
triton==2.0.0->torch>=1.13.0->peft==0.2.0) (3.25.2)
Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from
triton==2.0.0->torch>=1.13.0->peft==0.2.0) (16.0.5)
Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in
/usr/local/lib/python3.10/dist-packages (from transformers->peft==0.2.0) (0.15.1)
Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/distpackages (from transformers->peft==0.2.0) (2022.10.31)
Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from
transformers->peft==0.2.0) (2.27.1)
Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in
/usr/local/lib/python3.10/dist-packages (from transformers->peft==0.2.0) (0.13.3)
Requirement already satisfied: Looking in indexes: https://pypi.org/simple, https://us
python.pkg.dev/colab-wheels/public/simple/
Requirement already satisfied: peft==0.2.0 in /usr/local/lib/python3.10/dist-packages
(0.2.0)
Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from
peft==0.2.0) (1.22.4)
Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages
(from peft==0.2.0) (23.1)
Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from
peft==0.2.0) (5.9.5)
Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from
peft==0.2.0) (6.0)
Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages
(from peft==0.2.0) (2.0.1+cu118)
Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages
(from peft==0.2.0) (4.27.2)
Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from
peft==0.2.0) (0.17.1)
Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from
torch>=1.13.0->peft==0.2.0) (3.12.0)
Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/distpackages (from torch>=1.13.0->peft==0.2.0) (4.5.0)
Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from
torch>=1.13.0->peft==0.2.0) (1.11.1)
Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from
torch>=1.13.0->peft==0.2.0) (3.1)
Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from
torch>=1.13.0->peft==0.2.0) (3.1.2)
Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages
(from torch>=1.13.0->peft==0.2.0) (2.0.0)
Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from
triton==2.0.0->torch>=1.13.0->peft==0.2.0) (3.25.2)
Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from
triton==2.0.0->torch>=1.13.0->peft==0.2.0) (16.0.5)
Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in
/usr/local/lib/python3.10/dist-packages (from transformers->peft==0.2.0) (0.15.1)
Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/distpackages (from transformers->peft==0.2.0) (2022.10.31)
Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from
transformers->peft==0.2.0) (2.27.1)
Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in
/usr/local/lib/python3.10/dist-packages (from transformers->peft==0.2.0) (0.13.3)
Requirement already sati /lib/python3.10/dist-packages (from google-auth<3,>=1.6.3-
>tensorboard) (0.3.0)
Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages
(from google-auth<3,>=1.6.3->tensorboard) (4.9)
Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/distpackages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/distpackages (from requests<3,>=2.21.0->tensorboard) (1.26.15)
Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-
packages (from requests<3,>=2.21.0->tensorboard) (2022.12.7)
Requirement already satisfied: charset-normalizer~=2.0.0 in
/usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.12)
Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages
(from requests<3,>=2.21.0->tensorboard) (3.4)
Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/distpackages (from werkzeug>=1.0.1->tensorboard) (2.1.2)
Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk-
>rouge-score) (8.1.3)
Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from
nltk->rouge-score) (1.2.0)
Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages
(from nltk->rouge-score) (2022.10.31)
Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk-
>rouge-score) (4. 65.0)
Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/distpackages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.0)
Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages
(from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)

2. Load and prepare a dataset
we will use the samsum dataset, a collection of about 16k messenger-like conversations with
summaries. Conversations were created and written down by linguists fluent in English.
Dataset:
{
 "id": "13818513",
 "summary": "Amanda baked cookies and will bring Jerry some tomorrow.",
 "dialogue": "Amanda: I baked cookies. Do you want some?\r\nJerry:
Sure!\r\nAmanda: I'll bring you tomorrow :-)"
}
To load the samsum dataset, we use the load_dataset() method from the Datasets library.
from datasets import load_dataset
# Load dataset from the hub
dataset = load_dataset("samsum")
print(f"Train dataset size: {len(dataset['train'])}") print(f"Test dataset size: {len(dataset['test'])}")
# Train dataset size: 14732 #
Test dataset size: 819
WARNING:datasets.builder:Found cached dataset samsum
(/root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef7
0d1277109031327bc9cc6af5d3d46e)
0%| | 0/3 [00:00<?, ?it/s]
Train dataset size: 14732
Test dataset size: 819
To train our model, we need to convert our inputs (text) to token IDs. This is done by a Transformers
Tokenizer.
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
model_id="google/flan-t5-xxl"
# Load tokenizer of FLAN-t5-XL
tokenizer = AutoTokenizer.from_pretrained(model_id)
Before we can start training, we need to preprocess our data. Abstractive Summarization is a text-generation
task. Our model will take a text as input and generate a summary as output. We want to understand how long
our input and output will take to batch our data efficiently.
from datasets import concatenate_datasets
import numpy as np
# The maximum total input sequence length after tokenization.
# Sequences longer than this will be truncated, sequences shorter will be padded.
tokenized_inputs = concatenate_datasets([dataset["train"], dataset["test"]]).map(lambda x: tokenizer(x["dialogue"],
truncation=True), batched=True, remove_columns=["dialogue", "summary"]) input_lenghts = [len(x) for x in
tokenized_inputs["input_ids"]]
# take 85 percentile of max length for better utilization
max_source_length = int(np.percentile(input_lenghts, 85))
print(f"Max source length: {max_source_length}")
# The maximum total sequence length for target text after tokenization
. # Sequences longer than this will be truncated, sequences shorter will be padded."
tokenized_targets = concatenate_datasets([dataset["train"], dataset["test"]]).map(lambda x: tokenizer(x["summary"],
truncation=True), batched=True, remove_columns=["dialogue", "summary"])
target_lenghts = [len(x) for x in tokenized_targets["input_ids"]]
# take 90 percentile of max length for better utilization
max_target_length = int(np.percentile(target_lenghts, 90))
print(f"Max target length: {max_target_length}")
WARNING:datasets.arrow_dataset:Loading cached processed dataset at
/root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70
d1277109031327bc9cc6af5d3d46e/cache-28f9652739780da5.arrow
Max source length: 255
0%| | 0/16 [00:00<?, ?ba/s]
Max target length: 50
Here, I have preprocessed our dataset before training and save it to disk. You could run this step on your
local machine or a CPU and upload it too the HuggingFacehub
def preprocess_function(sample,padding="max_length"):
# add prefix to the input for t5
inputs = ["summarize: " + item for item in sample["dialogue"]]
# tokenize inputs
model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)
# Tokenize targets with the `text_target` keyword argument
labels = tokenizer(text_target=sample["summary"], max_length=max_target_length, padding=padding,
truncation=True)
# If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore
# padding in the loss.
if padding == "max_length":
 labels["input_ids"] = [
 [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels["input_ids"]
 ]
model_inputs["labels"] = labels["input_ids"]
return model_inputs
tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=["dialogue",
"summary", "id"])
print(f"Keys of tokenizedd dataset: {list(tokenized_dataset['train'].features)}")
# save datasets to disk for later easy loading
tokenized_dataset["train"].save_to_disk("data/train")
tokenized_dataset["test"].save_to_disk("data/eval")
WARNING:datasets.arrow_dataset:Loading cached processed dataset at
/root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70
d1277109031327bc9cc6af5d3d46e/cache-843fdeabfea77c89.arrow
0%| | 0/1 [00:00<?, ?ba/s]
WARNING:datasets.arrow_dataset:Loading cached processed dataset at
/root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70
d1277109031327bc9cc6af5d3d46e/cache-8aa88cb10112f4aa.arrow
Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']
LLM-Finetuning with PEFT

Saving the dataset (0/1 shards): 0%| | 0/14732 [00:00<?, ? examples/s]
Saving the dataset (0/1 shards): 0%| | 0/819 [00:00<?, ? examples/s]

3. Fine-Tune T5 with LoRA and bnb int-8
In addition to the LoRA technique, we will use bitsanbytes.LLM.int8() to quantize out frozen LLM to int8.
This allows us to reduce the needed memory for FLAN-T5 XXL ~4x.
The first step of our training is to load the model. We are going to use philschmid/flan-t5-xxl-sharded-fp16,
which is a sharded version of google/flan-t5-xxl, the sharding will help us to not run off of memory when
loading the model.
from transformers import AutoModelForSeq2SeqLM
# huggingface hub model id
model_id = "philschmid/flan-t5-xxl-sharded-fp16"
# load model from the hub
model = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=True, device_map="auto")
Now, we can prepare our model for the LoRA int-8 training using peft.
from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType
# Define LoRA Config
lora_config = LoraConfig(
r=16,
lora_alpha=32,
target_modules=["q", "v"],
lora_dropout=0.05,
bias="none",
task_type=TaskType.SEQ_2_SEQ_LM )
# prepare int-8 model for training
model = prepare_model_for_int8_training(model)
# add LoRA adaptor
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# trainable params: 18874368 || all params: 11154206720 || trainable%: 0.16921300163961817

As you can see, here we are only training 0.16% of the parameters of the model! This huge memory gain will
enable us to fine-tune the model without memory issues.
Next is to create a DataCollator that will take care of padding our inputs and labels. We will use the
DataCollatorForSeq2Seq from the Transformers library.
from transformers import DataCollatorForSeq2Seq
# we want to ignore tokenizer pad token in the loss
label_pad_token_id = -100
# Data collator
data_collator= DataCollatorForSeq2Seq(
 tokenizer,
 model=model,
 label_pad_token_id=label_pad_token_id,
 pad_to_multiple_of=8
)
The last step is to define the hyperparameters (TrainingArguments) we want to use for our training.
from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments
output_dir="lora-flan-t5-xxl"
# Define training args
training_args = Seq2SeqTrainingArguments(
 output_dir=output_dir,
 auto_find_batch_size=True,
 learning_rate=1e-3, # higher learning rate
 num_train_epochs=5,
 logging_dir=f"{output_dir}/logs",
logging_strategy="steps",
logging_steps=500,
save_strategy="no",
report_to="tensorboard",
)
# Create Trainer instance
trainer=Seq2SeqTrainer(
 model=model,
 args=training_args,
 data_collator=data_collator,
 train_dataset=tokenized_dataset["train"],
 )
model.config.use_cache = False # silence the warnings. Please re-enable for inference!

Let's now train our model and run the cells below. Note that for T5, some layers are kept in float32 for stability
purposes.
# Save our LoRA model & tokenizer results
peft_model_id="results"
trainer.model.save_pretrained(peft_model_id)
tokenizer.save_pretrained(peft_model_id)
# if you want to save the base model to call
# trainer.model.base_model.save_pretrained(peft_model_id)

Here, our LoRA checkpoint is only 84MB small and includes all of the learnt knowleddge for samsum.
4. Evaluate and run Interence with LoRA FLAN-T5
We are going to use evaluate library to evaluate the rogue score. We can run inference using PEFT and
transformers. For our FLAN-T5 XXL model, we need at least 18GB of GPU memory.
import torch
from peft import PeftModel, PeftConfig
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
# Load peft config for pre-trained checkpoint etc.
peft_model_id = "results"
config = PeftConfig.from_pretrained(peft_model_id)
#loadbase LLM model and tokenizer
model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path, load_in_8bit=True,
device_map={"":0})
tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)
# Load the Lora model
model = PeftModel.from_pretrained(model, peft_model_id, device_map={"":0})
model.eval()
print("Peft model loaded")
Let’s load the dataset again with a random sample to try the summarization.
from datasets import load_dataset
from random import randrange
# Load dataset from the hub and get a sample
dataset = load_dataset("samsum")

sample = dataset['test'][randrange(len(dataset["test"]))]
input_ids = tokenizer(sample["dialogue"], return_tensors="pt", truncation=True).input_ids.cuda()
# with torch.inference_mode():
outputs = model.generate(input_ids=input_ids, max_new_tokens=10, do_sample=True, top_p=0.9)
print(f"input sentence: {sample['dialogue']}\n{'---'* 20}")
print(f"summary:\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]}")
Nice! Now this model works! Now, let's take a closer look and evaluate it against the test set of processed
datasets from samsum. Therefore, we need to use and create some utilities to generate the summaries and group
them together. The most commonly used metrics to evaluate summarization task is rogue_score short for
Recall-Oriented Understudy for Gisting Evaluation). This metric does not behave like the standard accuracy: it
will compare a generated summary against a set of reference summaries.
import evaluate
import numpy as np
from datasets import load_from_disk
from tqdm import tqdm
# Metric
metric = evaluate.load("rouge")
def evaluate_peft_model(sample,max_target_length=50):
 # generate summary
outputs = model.generate(input_ids=sample["input_ids"].unsqueeze(0).cuda(), do_sample=True, top_p=0.9,
max_new_tokens=max_target_length)
 prediction = tokenizer.decode(outputs[0].detach().cpu().numpy(), skip_special_tokens=True)
# decode eval sample
# Replace -100 in the labels as we can't decode them.
labels = np.where(sample['labels'] != -100, sample['labels'], tokenizer.pad_token_id)
labels = tokenizer.decode(labels, skip_special_tokens=True)
# Some simple post-processing
return prediction, labels
# load test dataset from distk
test_dataset = load_from_disk("data/eval/").with_format("torch")
# run predictions
# this can take ~45 minutes
predictions, references = [] , []
for sample in tqdm(test_dataset):
 p,l = evaluate_peft_model(sample)
 predictions.append(p)
 references.append(l)
# compute metric
rogue = metric.compute(predictions=predictions, references=references, use_stemmer=True)
# print results
print(f"Rogue1: {rogue['rouge1']* 100:2f}%")
print(f"rouge2: {rogue['rouge2']* 100:2f}%")

print(f"rougeL: {rogue['rougeL']* 100:2f}%")
print(f"rougeLsum: {rogue['rougeLsum']* 100:2f}%")
# Rogue1: 50.386161%
# rouge2: 24.842412%
# rougeL: 41.370130%
# rougeLsum: 41.394230%
The PEFT fine-tuned FLAN-T5-XXL achieved a rogue1 score of 50.38% on the test dataset. For comparison a
full fine-tuning of flan-t5-base achieved a rouge1 score of 47.233, That is 3% improvements.
This LoRA checkpoint is only 84MB small and model achieves better performance than a smaller fully finetuned model.

Fine-Tune Your Own Llama 2 Model in a Colab Notebook

A practical introduction to LLM fine-tuning

Background on fine-tuning LLMs

Summary:
1. LLM Pretraining:
• Large Language Models (LLMs) are pretrained on extensive text corpora.
• Llama 2 was pretrained on a dataset of 2 trillion tokens, compared to BERT's training on
BookCorpus and Wikipedia.
• Pretraining is resource-intensive and time-consuming.
2. Auto-Regressive Prediction:
• Llama 2, an auto-regressive model, predicts the next token in a sequence.
• Auto-regressive models lack usefulness in providing instructions, leading to the need for
instruction tuning.
3. Fine-Tuning Techniques:
• Instruction tuning uses two main fine-tuning techniques: a. Supervised Fine-Tuning (SFT):
Trained on instruction-response datasets, minimizing differences between generated and
actual responses. b. Reinforcement Learning from Human Feedback (RLHF): Trained to
maximize rewards based on human evaluations.
4. RLHF vs. SFT:
• RLHF captures complex human preferences but requires careful reward system design and
consistent human feedback.
• Direct Preference Optimization (DPO) might be a future alternative to RLHF.
• SFT can be highly effective when the model hasn't encountered specific data during
pretraining.
5. Effective SFT Example:
• LIMA paper showed improved performance of LLaMA v1 model over GPT-3 by finetuning on a small high-quality dataset.
• Data quality and model size (e.g., 65b parameters) are crucial for successful fine-tuning.
6. Importance of Prompt Templates:
Prompt templates structure inputs: system prompt, user prompt, additional inputs, and model
answer.
Llama 2's template example: [INST] <> System prompt <> User prompt [/INST] Model answer
Different templates (e.g., Alpaca, Vicuna) have varying impacts.
7. Reformatting for Llama 2:
Converting instruction dataset to Llama 2's template is important.
The tutorial author already reformatted a dataset for this purpose.
 8. Base Llama 2 Model vs. Chat Version
 Specific prompt templates not necessary for base Llama 2 model, unlike the chat version.
(Note: LLMs = Large Language Models, SFT = Supervised Fine-Tuning, RLHF = Reinforcement Learning
from Human Feedback, DPO = Direct Preference Optimization)
Fine-Tuning Llama 2 (7 billion parameters) with VRAM Limitations and QLoRA:
In this section, the goal is to fine-tune a Llama 2 model with 7 billion parameters using a T4 GPU with 16 GB
of VRAM. Given the VRAM limitations, traditional fine-tuning is not feasible, necessitating parameter-

efficient fine-tuning (PEFT) techniques like LoRA or QLoRA. The chosen approach is QLoRA, which
employs 4-bit precision to drastically reduce VRAM usage.
The following steps will be executed:
1. Environment Setup:
a. The task involves leveraging the Hugging Face ecosystem and several libraries: transformers,
accelerate, peft, trl, and bitsandbytes.
2. Installation and Library Loading:
a. The first step is to install and load the required libraries, as provided by Younes Belkada's
GitHub Gist.
(Note: T4 GPU has 16 GB VRAM, 7 billion parameters of Llama 2 in 4-bit precision consume around 14 GB
in FP16, and PEFT techniques like QLoRA are employed for efficient fine-tuning.)
# !pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7
# Import necessary packages for the fine-tuning process
import os # Operating system functionalities
import torch # PyTorch library for deep learning
from datasets import load_dataset # Loading datasets for training
from transformers import (
AutoModelForCausalLM, # AutoModel for language modeling tasks
 AutoTokenizer, # AutoTokenizer for tokenization
BitsAndBytesConfig, # Configuration for BitsAndBytes
HfArgumentParser, # Argument parser for Hugging Face models
TrainingArguments, # Training arguments for model training
Pipeline , # Creating pipelines for model inference
logging, # Logging information during training
)
from peft import LoraConfig, PeftModel # Packages for parameter-efficient fine-tuning (PEFT) from trl
import SFTTrainer # SFTTrainer for supervised fine-tuning
# !pip install -q datasets
!huggingface-cli login
 _| _| _| _| _|_|_| _|_|_| _|_|_| _| _| _|_|_| _|_|_|_|
_|_| _|_|_| _|_|_|_|
 _| _| _| _| _| _| _| _|_| _| _| _|
_| _| _| _|
 _|_|_|_| _| _| _| _|_| _| _|_| _| _| _| _| _| _|_| _|_|_|
_|_|_|_| _| _|_|_|
 _| _| _| _| _| _| _| _| _| _| _|_| _| _| _|
_| _| _| _|
 _| _| _|_| _|_|_| _|_|_| _|_|_| _| _| _|_|_| _|
_| _| _|_|_| _|_|_|_|

A token is already saved on your machine. Run `huggingface-cli whoami` to get more
information or `huggingface-cli logout` if you want to log out.
Setting a new token will erase the existing one.
 To login, `huggingface_hub` requires a token generated from
https://huggingface.co/settings/tokens .
Token:
Add token as git credential? (Y/n) n
Token is valid (permission: write).
Your token has been saved to /root/.cache/huggingface/token
Login successful
• Section 1: Parameters to tune
o Load a llama-2-7b-chat-hf model and train it on the mlabonne/guanaco-llama2-1k dataset.
o The dataset contains 1,000 samples.
o You can find more information about the dataset in this notebook.
o Feel free to use a different dataset.
• Section 2: QLoRA parameters
o QLoRA will use a rank of 64 with a scaling parameter of 16.
o See this article for more information about LoRA parameters.
o The Llama 2 model will be loaded directly in 4-bit precision using the NF4 type.
o The model will be trained for one epoch.
• Section 3: Other parameters
o To get more information about the other parameters, check the TrainingArguments,
PeftModel, and SFTTrainer documentation.
# The model that you want to train from the Hugging Face hub
model_name = "NousResearch/Llama-2-7b-hf"
# The instruction dataset to use

dataset_name = "mlabonne/guanaco-llama2-1k"
# Fine-tuned model name
new_model = "llama-2-7b-miniguanaco"
################################################################################
# QLoRA parameters
################################################################################
# LoRA attention dimension
lora_r = 64
# Alpha parameter for LoRA scaling
lora_alpha = 16
# Dropout probability for LoRA layers
lora_dropout = 0.1
################################################################################
# bitsandbytes parameters
###########################################################################
# Activate 4-bit precision base model loading
use_4bit = True
# Compute dtype for 4-bit base models
bnb_4bit_compute_dtype = "float16"
# Quantization type (fp4 or nf4)
bnb_4bit_quant_type = "nf4"
# Activate nested quantization for 4-bit base models (double quantization)
use_nested_quant = False
################################################################################
# TrainingArguments parameters
################################################################################
# Output directory where the model predictions and checkpoints will be stored
output_dir = "./results"
# Number of training epochs
num_train_epochs = 1
# Enable fp16/bf16 training (set bf16 to True with an A100)
fp16 = False
bf16 = False
# Batch size per GPU for training
per_device_train_batch_size = 4
# Number of update steps to accumulate the gradients for .
gradient_accumulation_steps = 1
# Enable gradient checkpointing
gradient_checkpointing = True
# Maximum gradient normal (gradient clipping)
max_grad_norm = 0.3
# Initial learning rate (AdamW optimizer)
learning_rate = 2e-4
# Weight decay to apply to all layers except bias/LayerNorm weights
weight_decay = 0.001
# Optimizer to use
optim = "paged_adamw_32bit"
# Learning rate schedule (constant a bit better than cosine)
lr_scheduler_type = "constant"
# Number of training steps (overrides num_train_epochs)
max_steps = -1
# Ratio of steps for a linear warmup (from 0 to learning rate)
warmup_ratio = 0.03
# Group sequences into batches with same length
# Saves memory and speeds up training considerably
group_by_length = True
# Save checkpoint every X updates steps
save_steps = 25
# Log every X updates steps
logging_steps = 25
################################################################################
# SFT parameters
################################################################################
# Maximum sequence length to use
max_seq_length = None
# Pack multiple short examples in the same input sequence to increase efficiency
packing = False
# Load the entire model on the GPU 0
device_map = {"": 0}
1. Loading the Dataset: The first step involves loading the preprocessed dataset. This dataset will be used
for fine-tuning. Preprocessing might involve reformatting prompts, filtering out low-quality text, and
combining multiple datasets if needed.
2. Configuring BitsAndBytes for 4-bit Quantization: The BitsAndBytesConfig is set up to enable 4-bit
quantization. This configuration is crucial for reducing the memory usage during fine-tuning.
3. Loading Llama 2 Model and Tokenizer in 4-bit Precision: The Llama 2 model is loaded with 4-bit
precision, which significantly reduces the memory footprint. The corresponding tokenizer is also loaded
to preprocess the text data.
 4. Loading Configurations and Initializing SFTTrainer:
• The configurations needed for QLoRA, which is a parameter-efficient fine-tuning technique, are
loaded.
• Regular training parameters are set up.
• The SFTTrainer is initialized with all the loaded configurations and
parameters. This trainer will manage the supervised fine-tuning process.
5. Start of Training: After all the necessary components are loaded and configured, the training process
begins. The SFTTrainer takes care of fine-tuning the Llama 2 model using the specified dataset,
configurations, and parameters.
These steps collectively set up the environment for fine-tuning a Llama 2 model with 7 billion parameters in 4-
bit precision using the QLoRA technique, thus optimizing for VRAM limitations while maintaining model
performance.
# Step 1 : Load dataset (you can process it here)
dataset = load_dataset(dataset_name, split="train")
# Step 2 :Load tokenizer and model with QLoRA configuration
compute_dtype = getattr(torch, bnb_4bit_compute_dtype)
bnb_config = BitsAndBytesConfig(
load_in_4bit=use_4bit,
bnb_4bit_quant_type=bnb_4bit_quant_type,
bnb_4bit_compute_dtype=compute_dtype,
bnb_4bit_use_double_quant=use_nested_quant, )
# Step 3 :Check GPU compatibility with bfloat16
if compute_dtype == torch.float16 and use_4bit:
major, _ = torch.cuda.get_device_capability()
if major >= 8:
print("=" * 80)
print("Your GPU supports bfloat16: accelerate training with bf16=True")
print("=" * 80)
# Step 4 :Load base model
model = AutoModelForCausalLM.from_pretrained(
model_name,
quantization_config=bnb_config,
device_map=device_map
)
model.config.use_cache = False
model.config.pretraining_tp = 1
Downloading (…)lve/main/config.json: 0%| | 0.00/583 [00:00<?, ?B/s]
Downloading (…)fetensors.index.json: 0%| | 0.00/26.8k [00:00<?, ?B/s]
Downloading shards: 0%| | 0/2 [00:00<?, ?it/s]
Downloading (…)of-00002.safetensors: 0%| | 0.00/9.98G [00:00<?, ?B/s]
Downloading (…)of-00002.safetensors: 0%| | 0.00/3.50G [00:00<?, ?B/s]
Loading checkpoint shards: 0%| | 0/2 [00:00<?, ?it/s]
Downloading (…)neration_config.json: 0%| | 0.00/179 [00:00<?, ?B/s]
# Step 5 :Load LLaMA tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.add_special_tokens({'pad_token': '[PAD]'})
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"
Downloading (…)okenizer_config.json: 0%| | 0.00/746 [00:00<?, ?B/s]
Downloading tokenizer.model: 0%| | 0.00/500k [00:00<?, ?B/s]
Downloading (…)/main/tokenizer.json: 0%| | 0.00/1.84M [00:00<?, ?B/s]
Downloading (…)in/added_tokens.json: 0%| | 0.00/21.0 [00:00<?, ?B/s]
Downloading (…)cial_tokens_map.json: 0%| | 0.00/435 [00:00<?, ?B/s]
# Step 6 :Load LoRA configuration
peft_config = LoraConfig(
lora_alpha=lora_alpha,
lora_dropout=lora_dropout,
r=lora_r,
bias="none",
task_type="CAUSAL_LM", )
# Step 7 :Set training parameters
training_arguments = TrainingArguments(
output_dir=output_dir,
num_train_epochs=num_train_epochs,
per_device_train_batch_size=per_device_train_batch_size,
gradient_accumulation_steps=gradient_accumulation_steps,
optim=optim,
save_steps=save_steps,
logging_steps=logging_steps,
learning_rate=learning_rate,
weight_decay=weight_decay,
fp16=fp16,
bf16=bf16,
max_grad_norm=max_grad_norm,
max_steps=max_steps,
warmup_ratio=warmup_ratio,
group_by_length=group_by_length,
lr_scheduler_type=lr_scheduler_type,
report_to="tensorboard”
)
# Step 8 :Set supervised fine-tuning parameters
trainer = SFTTrainer(
model=model,
train_dataset=dataset,
peft_config=peft_config,
dataset_text_field="text",
max_seq_length=max_seq_length,
tokenizer=tokenizer,
args=training_arguments,
packing=packing,
)
/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:102: FutureWarning:
prepare_model_for_int8_training is deprecated and will be removed in a future version.

Use prepare_model_for_kbit_training instead.
 warnings.warn(
/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:159: UserWarning: You
didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024
 warnings.warn(
Map: 0%| | 0/1000 [00:00<?, ? examples/s]
# Step 9 :Train model
trainer.train()
# Step 10 :Save trained model
trainer.model.save_pretrained(new_model)
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer,
using the `__call__` method is faster than using a method to encode the text followed by
a call to the `pad` method to get a padded encoding.
[250/250 26:14, Epoch 1/1]
Step Training Loss
25 1.4355775
50 1.5657964
75 1.5461145
100 1.5555535
125 1.7675787
150 1.7675778
175 1.5345467
200 1.6775677
225 1.7676556
250 1.5685657
%load_ext tensorboard
%tensorboard --logdir results/runs

